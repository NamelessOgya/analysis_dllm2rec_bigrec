VERSION: 0.12.0
SIGNATURE: (n: int = 1, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, repetition_penalty: float = 1.0, temperature: float = 1.0, top_p: float = 1.0, top_k: int = 0, min_p: float = 0.0, seed: int | None = None, stop: str | list[str] | None = None, stop_token_ids: list[int] | None = None, ignore_eos: bool = False, max_tokens: int | None = 16, min_tokens: int = 0, logprobs: int | None = None, prompt_logprobs: int | None = None, flat_logprobs: bool = False, detokenize: bool = True, skip_special_tokens: bool = True, spaces_between_special_tokens: bool = True, logits_processors: Optional[Any] = None, include_stop_str_in_output: bool = False, truncate_prompt_tokens: Optional[Annotated[int, msgspec.Meta(ge=-1)]] = None, output_kind: vllm.sampling_params.RequestOutputKind = <RequestOutputKind.CUMULATIVE: 0>, output_text_buffer_length: int = 0, _all_stop_token_ids: set[int] = <factory>, structured_outputs: vllm.sampling_params.StructuredOutputsParams | None = None, logit_bias: dict[int, float] | None = None, allowed_token_ids: list[int] | None = None, extra_args: dict[str, typing.Any] | None = None, bad_words: list[str] | None = None, _bad_words_token_ids: list[list[int]] | None = None, skip_reading_prefix_cache: bool | None = None)
DOC: Sampling parameters for text generation.

    Overall, we follow the sampling parameters from the OpenAI text completion
    API (https://platform.openai.com/docs/api-reference/completions/create).
    In addition, we support beam search, which is not supported by OpenAI.
    
