Help on class SamplingParams in module vllm.sampling_params:

class SamplingParams(vllm.v1.serial_utils.PydanticMsgspecMixin, msgspec.Struct)
 |  SamplingParams(n: int = 1, presence_penalty: float = 0.0, frequency_penalty: float = 0.0, repetition_penalty: float = 1.0, temperature: float = 1.0, top_p: float = 1.0, top_k: int = 0, min_p: float = 0.0, seed: int | None = None, stop: str | list[str] | None = None, stop_token_ids: list[int] | None = None, ignore_eos: bool = False, max_tokens: int | None = 16, min_tokens: int = 0, logprobs: int | None = None, prompt_logprobs: int | None = None, flat_logprobs: bool = False, detokenize: bool = True, skip_special_tokens: bool = True, spaces_between_special_tokens: bool = True, logits_processors: Optional[Any] = None, include_stop_str_in_output: bool = False, truncate_prompt_tokens: Optional[Annotated[int, msgspec.Meta(ge=-1)]] = None, output_kind: vllm.sampling_params.RequestOutputKind = <RequestOutputKind.CUMULATIVE: 0>, output_text_buffer_length: int = 0, _all_stop_token_ids: set[int] = <factory>, structured_outputs: vllm.sampling_params.StructuredOutputsParams | None = None, logit_bias: dict[int, float] | None = None, allowed_token_ids: list[int] | None = None, extra_args: dict[str, typing.Any] | None = None, bad_words: list[str] | None = None, _bad_words_token_ids: list[list[int]] | None = None, skip_reading_prefix_cache: bool | None = None)
 |  
 |  Sampling parameters for text generation.
 |  
 |  Overall, we follow the sampling parameters from the OpenAI text completion
 |  API (https://platform.openai.com/docs/api-reference/completions/create).
 |  In addition, we support beam search, which is not supported by OpenAI.
 |  
 |  Method resolution order:
 |      SamplingParams
 |      vllm.v1.serial_utils.PydanticMsgspecMixin
 |      msgspec.Struct
 |      msgspec._core._StructMixin
 |      builtins.object
 |  
 |  Methods defined here:
 |  
 |  __post_init__(self) -> None
 |  
 |  __repr__(self) -> str
 |      Return repr(self).
 |  
 |  clone(self) -> 'SamplingParams'
 |      Deep copy, but maybe not the LogitsProcessor objects.
 |      
 |      LogitsProcessor objects may contain an arbitrary, nontrivial amount of
 |      data that is expensive to copy. However, if not copied, the processor
 |      needs to support parallel decoding for multiple sequences
 |      See https://github.com/vllm-project/vllm/issues/3087
 |  
 |  sampling_type = <functools.cached_property object>
 |  update_from_generation_config(self, generation_config: dict[str, typing.Any], model_eos_token_id: int | None = None) -> None
 |      Update if there are non-default values from generation_config
 |  
 |  update_from_tokenizer(self, tokenizer: vllm.tokenizers.protocol.TokenizerLike) -> None
 |  
 |  ----------------------------------------------------------------------
 |  Static methods defined here:
 |  
 |  from_optional(n: int | None = 1, presence_penalty: float | None = 0.0, frequency_penalty: float | None = 0.0, repetition_penalty: float | None = 1.0, temperature: float | None = 1.0, top_p: float | None = 1.0, top_k: int = 0, min_p: float = 0.0, seed: int | None = None, stop: str | list[str] | None = None, stop_token_ids: list[int] | None = None, bad_words: list[str] | None = None, include_stop_str_in_output: bool = False, ignore_eos: bool = False, max_tokens: int | None = 16, min_tokens: int = 0, logprobs: int | None = None, prompt_logprobs: int | None = None, detokenize: bool = True, skip_special_tokens: bool = True, spaces_between_special_tokens: bool = True, logits_processors: list[collections.abc.Callable[[list[int], torch.Tensor], torch.Tensor] | collections.abc.Callable[[list[int], list[int], torch.Tensor], torch.Tensor]] | None = None, truncate_prompt_tokens: Optional[Annotated[int, msgspec.Meta(ge=-1)]] = None, output_kind: vllm.sampling_params.RequestOutputKind = <RequestOutputKind.CUMULATIVE: 0>, structured_outputs: vllm.sampling_params.StructuredOutputsParams | None = None, logit_bias: dict[int, float] | dict[str, float] | None = None, allowed_token_ids: list[int] | None = None, extra_args: dict[str, typing.Any] | None = None) -> 'SamplingParams'
 |  
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |  
 |  all_stop_token_ids
 |  
 |  bad_words_token_ids
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors defined here:
 |  
 |  allowed_token_ids
 |  
 |  bad_words
 |  
 |  detokenize
 |  
 |  extra_args
 |  
 |  flat_logprobs
 |  
 |  frequency_penalty
 |  
 |  ignore_eos
 |  
 |  include_stop_str_in_output
 |  
 |  logit_bias
 |  
 |  logits_processors
 |  
 |  logprobs
 |  
 |  max_tokens
 |  
 |  min_p
 |  
 |  min_tokens
 |  
 |  n
 |  
 |  output_kind
 |  
 |  output_text_buffer_length
 |  
 |  presence_penalty
 |  
 |  prompt_logprobs
 |  
 |  repetition_penalty
 |  
 |  seed
 |  
 |  skip_reading_prefix_cache
 |  
 |  skip_special_tokens
 |  
 |  spaces_between_special_tokens
 |  
 |  stop
 |  
 |  stop_token_ids
 |  
 |  structured_outputs
 |  
 |  temperature
 |  
 |  top_k
 |  
 |  top_p
 |  
 |  truncate_prompt_tokens
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |  
 |  __annotations__ = {'_all_stop_token_ids': set[int], '_bad_words_token_...
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from vllm.v1.serial_utils.PydanticMsgspecMixin:
 |  
 |  __get_pydantic_core_schema__(source_type: Any, handler: pydantic.annotated_handlers.GetCoreSchemaHandler) -> Union[pydantic_core.core_schema.InvalidSchema, pydantic_core.core_schema.AnySchema, pydantic_core.core_schema.NoneSchema, pydantic_core.core_schema.BoolSchema, pydantic_core.core_schema.IntSchema, pydantic_core.core_schema.FloatSchema, pydantic_core.core_schema.DecimalSchema, pydantic_core.core_schema.StringSchema, pydantic_core.core_schema.BytesSchema, pydantic_core.core_schema.DateSchema, pydantic_core.core_schema.TimeSchema, pydantic_core.core_schema.DatetimeSchema, pydantic_core.core_schema.TimedeltaSchema, pydantic_core.core_schema.LiteralSchema, pydantic_core.core_schema.MissingSentinelSchema, pydantic_core.core_schema.EnumSchema, pydantic_core.core_schema.IsInstanceSchema, pydantic_core.core_schema.IsSubclassSchema, pydantic_core.core_schema.CallableSchema, pydantic_core.core_schema.ListSchema, pydantic_core.core_schema.TupleSchema, pydantic_core.core_schema.SetSchema, pydantic_core.core_schema.FrozenSetSchema, pydantic_core.core_schema.GeneratorSchema, pydantic_core.core_schema.DictSchema, pydantic_core.core_schema.AfterValidatorFunctionSchema, pydantic_core.core_schema.BeforeValidatorFunctionSchema, pydantic_core.core_schema.WrapValidatorFunctionSchema, pydantic_core.core_schema.PlainValidatorFunctionSchema, pydantic_core.core_schema.WithDefaultSchema, pydantic_core.core_schema.NullableSchema, pydantic_core.core_schema.UnionSchema, pydantic_core.core_schema.TaggedUnionSchema, pydantic_core.core_schema.ChainSchema, pydantic_core.core_schema.LaxOrStrictSchema, pydantic_core.core_schema.JsonOrPythonSchema, pydantic_core.core_schema.TypedDictSchema, pydantic_core.core_schema.ModelFieldsSchema, pydantic_core.core_schema.ModelSchema, pydantic_core.core_schema.DataclassArgsSchema, pydantic_core.core_schema.DataclassSchema, pydantic_core.core_schema.ArgumentsSchema, pydantic_core.core_schema.ArgumentsV3Schema, pydantic_core.core_schema.CallSchema, pydantic_core.core_schema.CustomErrorSchema, pydantic_core.core_schema.JsonSchema, pydantic_core.core_schema.UrlSchema, pydantic_core.core_schema.MultiHostUrlSchema, pydantic_core.core_schema.DefinitionsSchema, pydantic_core.core_schema.DefinitionReferenceSchema, pydantic_core.core_schema.UuidSchema, pydantic_core.core_schema.ComplexSchema] from msgspec._core.StructMeta
 |      Make msgspec.Struct compatible with Pydantic, respecting defaults.
 |      Handle JSON=>msgspec.Struct. Used when exposing msgspec.Struct to the
 |      API as input or in `/docs`. Note this is cached by Pydantic and not
 |      called on every validation.
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from vllm.v1.serial_utils.PydanticMsgspecMixin:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from msgspec._core._StructMixin:
 |  
 |  __copy__(...)
 |      copy a struct
 |  
 |  __delattr__(self, name, /)
 |      Implement delattr(self, name).
 |  
 |  __eq__(self, value, /)
 |      Return self==value.
 |  
 |  __ge__(self, value, /)
 |      Return self>=value.
 |  
 |  __gt__(self, value, /)
 |      Return self>value.
 |  
 |  __hash__(self, /)
 |      Return hash(self).
 |  
 |  __le__(self, value, /)
 |      Return self<=value.
 |  
 |  __lt__(self, value, /)
 |      Return self<value.
 |  
 |  __ne__(self, value, /)
 |      Return self!=value.
 |  
 |  __reduce__(...)
 |      reduce a struct
 |  
 |  __replace__(...)
 |      create a new struct with replacements
 |  
 |  __rich_repr__(...)
 |      rich repr
 |  
 |  __setattr__(self, name, value, /)
 |      Implement setattr(self, name, value).
 |  
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from msgspec._core._StructMixin:
 |  
 |  __struct_config__ = <msgspec.structs.StructConfig object>
 |  
 |  __struct_defaults__ = (1, 0.0, 0.0, 1.0, 1.0, 1.0, 0, 0.0, None, None,...
 |  
 |  __struct_encode_fields__ = ('n', 'presence_penalty', 'frequency_penalt...
 |  
 |  __struct_fields__ = ('n', 'presence_penalty', 'frequency_penalty', 're...

